# -*- coding: utf-8 -*-
"""Python assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KAQs_R-fgU6N4fnp3oiw4a4w4Cx-MO3m

Import all necessary packages needed for the analysis
"""

#import necesssary packages needed for analysis
import pandas as pd
import numpy as np
from numpy.ma.extras import average
import re
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import style
style.use('ggplot')
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

#import dataset for use 
from google.colab import files
uploaded = files.upload()

# Store Dataset in a Pandas Dataframe
import io
df = pd.read_csv(io.BytesIO(uploaded['trump.csv']), parse_dates=['date'])

#carry out some analysis to understand the frame of the date e.g number of rows, data types etc.
print(df)

df.describe(include=object)

df.info()

print('Count of rows in the data is:  ', len(df))

df['isFlagged'].value_counts()

#check if the tweets have a url 
from urllib.parse import urlparse

# Define a function to count the number of URLs in each rows "text" column
def count_urls (row):
  text = row ['text']
  parsed = urlparse (text)
  if parsed. scheme and parsed.netloc:
    return 1
  else:
    return 0
#add the new column to the sheet
df['num_urls'] = df. apply (count_urls, axis=1)

#check the value of tweets that have urls and those that don't
df['num_urls'].value_counts()

#determine if a tweet has an image 
def contains_image (row):
  text = row['text']
  if 'pic.twitter.com' in text:
    return True
  else:
    return False

# Add a new column to the Dataframe to indicate if each tweet contains an image
df ['contains_image'] = df. apply (contains_image, axis=1)

#check the value of tweets that have images and those that don't
df['contains_image'].value_counts()

df['tweetlength'] = df['text'].apply(len)

#visualisation before data processing to show the trend of the number of likes gotten in a time series.
time_likes = pd.Series(data=df['likes'].values, index=df['date'])
time_likes.plot(figsize=(16, 4), label="likes", legend=True)

#since a high concentration of like is around 2019 and 2020, a more distinct visualisation is done to view the tweet frequency in those years 
# Create a new dataframe that only has tweets from year 2019 and 2020
df1 = df[df['date'].dt.year.isin([2019, 2020])]

# Convert the date column to a pandas DatetimeIndex and extract the year and hour components
year = pd.DatetimeIndex(df1['date']).year
hour = pd.DatetimeIndex(df1['date']).hour

# Categorize the hours into morning, afternoon, or evening bins
bins = [0, 11, 16, 23]
labels = ['morning', 'afternoon', 'evening']
time_of_day = pd.cut(hour, bins=bins, labels=labels)

# Group the tweets by year and time of day, and count the number of tweets in each group
grouped = df1.groupby([year, time_of_day])['text'].count().unstack()

# Create a stacked bar chart to visualize the tweet frequency by time of day and year
ax = grouped.plot(kind='bar', stacked=True, title='Tweet Frequency by Time of Day')
ax.set_xlabel('Time of Day')
ax.set_ylabel('Number of Tweets')
ax.legend(title='Year')
plt.show()

#carry out some more data cleaning before visualising the frequency of tweets in 2020 (the year with the most tweets concentration)
#get cuttoff date to remove dates older than 5 years ago 
from datetime import datetime, timedelta
cutoff_date = datetime.now() - timedelta(days=365*5)
print(cutoff_date)

#Keep only tweets fom 5 years ago and above in the dataframe only 
cutoff = '2018-04-22 19:14:12.208254'
df = df[df['date'] >= cutoff]

#check the number of rows left to see how the date cuttoff impacted on the dataset available.
print('Count of rows in the data is:  ', len(df))

#another cleaning process is done to keep only tweets that are equal to or more than 4 words 
df = df[df['text'].str.split().apply(len) >= 4]

#check the number of columns and rows left to see the dataset remaining.
print('Count of columns in the data is:  ', len(df.columns))
print('Count of rows in the data is:  ', len(df))

#check for the null values in the tweets
df.isnull().sum().sum()

#Now that the major data cleaning is completed, we can now carry out visualisation processes to determine the frequency of tweets
# Create a new dataframe to only keep tweets from year 2020 (This is the year with the most likes)
df2 = df[df['date'].dt.year == 2020]

# Convert the date column to a pandas DatetimeIndex and extract the hour component
hour = pd.DatetimeIndex(df2['date']).hour

# Categorize the hours into morning, afternoon, or evening bins
bins = [0, 11, 16, 23]
labels = ['morning', 'afternoon', 'evening']
time_of_day = pd.cut(hour, bins=bins, labels=labels)

# Group the tweets by time of day and count the number of tweets in each group
grouped = df2.groupby(time_of_day)['text'].count()

# Create a bar chart to visualize the tweet frequency by time of day
ax = grouped.plot(kind='bar', title='Tweet Frequency by Time of Day in 2020')
ax.set_xlabel('Time of Day')
ax.set_ylabel('Number of Tweets')
plt.show()

# Convert the date column to a pandas DatetimeIndex and extract the hour component
hour = pd.DatetimeIndex(df['date']).hour

# Categorize the hours into morning, afternoon, or evening bins
bins = [0, 11, 16, 23]
labels = ['morning', 'afternoon', 'evening']
time_of_day = pd.cut(hour, bins=bins, labels=labels)
df ['time'] = time_of_day
df ['hour'] = hour

# extract the day of the week (Monday=0, Sunday=6)
df['day'] = df['date'].dt.strftime('%w')

#check for the null values in the tweets
df.isnull().sum().sum()

df['time']=df['time'].astype ('category')

df.info()

df['time'].value_counts()

# Group the rows by hour and count the number of rows with 'nan' labels
nan_counts = df[df['time'].isnull()].groupby('hour').size()

# Plot the results as a bar chart
nan_counts.plot(kind='bar')
plt.xlabel('Hour')
plt.ylabel('Number of NaN labels')
plt.title('Number of NaN labels by hour')
plt.show()

print(df)

df['time'] = df['time'].fillna('morning')

# Group by hour and time of day
grouped = df.groupby(['hour', 'time']).size().reset_index(name='count')

# Pivot the table to create a matrix with time of day categories as columns
pivoted = grouped.pivot(index='hour', columns='time', values='count')

# Create a stacked bar chart
ax = pivoted.plot(kind='bar', stacked=True, figsize=(10, 6))

# Set the axis labels and title
ax.set_xlabel('Hour of Day')
ax.set_ylabel('Frequency')
ax.set_title('Tweet Frequency by Hour of Day')

# Show the plot
plt.show()

print(df)

#Also visualise the average likes per day for the entire dataset
avg_likes = df.groupby(df['date'].dt.date)['likes'].mean()

# Plot the average likes over time
fig, ax = plt.subplots()
ax.plot(avg_likes.index, avg_likes.values)
ax.set_xlabel('Date')
ax.set_ylabel('Average Likes')
ax.set_title('Average Likes per Day over a timeframe (2018 - 2021)')

plt.show()

#since the above graph shows the average likes to be highest in 2020, we plot another graph to cover just dataset with 2020 tweets
avg_likes = df2.groupby(df2['date'].dt.date)['likes'].mean()

# Plot the average likes over time
fig, ax = plt.subplots()
ax.plot(avg_likes.index, avg_likes.values)
ax.set_xlabel('Date')
ax.set_ylabel('Average Likes')
ax.set_title('Average Likes per Day in 2020')

plt.show()

#extra visualisation that shows the likes per month in the year 2020.
df2["month"] = pd.DatetimeIndex(df2['date']).month
sns.catplot(data= df2, x="month", y= 'likes' , kind="bar")

mostliked = df.loc[df.likes.nlargest(5).index]
mostliked

mostretweeted = df.loc[df.retweets.nlargest(5).index]
mostretweeted

#start data preprocessing before exploratory analysis so as to focus solely on the POI
#remove the RT as these are not directly quoted by the POI
df = df[~df.text.str.contains("RT")]

df['text']

print('Count of rows in the data is:  ', len(df))

import re
import string
import nltk
from nltk.corpus import stopwords

# Define a function to preprocess the text in a tweet
stop_words = set(stopwords.words('english'))
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'https?://\S+', '', text)

    # Remove numeric characters
    text = re.sub(r'\d+', '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Remove stopwords
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]

    # Join words back into a string
    text = ' '.join(filtered_words)

    return text

# Apply the preprocessing function to the 'text' column of the DataFrame
df['text'] = df['text'].apply(clean_text)

print(df)

# Drop duplicate tweets from the 'text' column
df.drop_duplicates(subset='text', keep='first', inplace=True)

print('Count of rows in the data is:  ', len(df))

#tokenize each text i.e make the text stand as individual words.
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\s+', gaps=True)
df['token'] = df['text'].apply(tokenizer.tokenize)
df['token'].head()

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

# Define a function to apply the stemmer to each token in a list
def stem_tokens(tokens):
    return [stemmer.stem(token) for token in tokens]

# Apply the stem_tokens() function to the 'text' column
df['token'] = df['token'].apply(stem_tokens)

print(df)

#Select out the most common words and count their frequencies 
from collections import Counter
common_words = Counter(" ".join(df["text"]).split()).most_common(10)
common_words

#Build wordcloud that shows the most popular words 
textt = ",".join(df.text)
wordcloud = WordCloud(scale=3, width=1500, height=800,max_words=50,colormap='RdYlGn',background_color="black", min_font_size=10).generate(textt)
plt.figure(figsize=(12,12))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.figure(1,figsize=(12, 12))
plt.title(' 50 Most Popular words in tweets',fontsize=19)
plt.show()

from PIL import Image
# Create an array from donald trump's image and use it as a mask
mask = np.array(Image.open('/content/hi/hi-PhotoRoom.png-PhotoRoom.png'))

mask

wc = WordCloud(background_color = 'white', mask = mask , contour_width = 2,
     contour_color = 'black', colormap='copper', scale = 3, min_font_size=10).generate(textt)
plt.figure(figsize=(15,15))
plt.axis("off")
plt.title('Donald Trump masked by his most popular words ',fontsize=19)
plt.imshow(wc)

#change the words in the tokenised and stemmed tweets into a string
df['token'] = df['token'].astype(str)

tokenn = ",".join(df.token)

df.info()

from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

analyzer= SentimentIntensityAnalyzer()
(df['TweetScore'])= df['token'].apply(analyzer.polarity_scores)
df.head()

def sentiment(row):
  text = row['TweetScore']
  polarity = "neutral"

  if(text['compound']>= 0.25):
    polarity = "positive"

  elif(text['compound']<= - 0.25):
    polarity = "negative"

  return polarity
df["Sentiment"] = df.apply(sentiment, axis= 1)
df['Compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['text']]

#show the sentiment value of the top 5 liked tweets.
df_top5= df.sort_values(by='likes', ascending=False).head(5)
df_top5

#plot bar chart to show distribution of sentiment value in the Top 5 tweets 
Top5 = df_top5.Sentiment.value_counts()
Top5.plot.barh()

#check the value count of sentiments in the entire tweet
df.Sentiment.value_counts()

#simple bar chart to show the sentiment value distribution in the dataset
sentiment_count = df.Sentiment.value_counts()
sentiment_count.plot.barh()

# Create a stacked bar chart of the sentiment analysis results
sns.set_style('whitegrid')
plt.bar(sentiment_count.index, sentiment_count.values, color=['red', 'blue', 'green'])
plt.title('Sentiment Analysis of Tweets by Trump')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

#pie chart to show sentiment distribution in the tweet
fig = plt.figure(figsize=(7,7))
colors = ("yellowgreen", "gold", "red")
wp = {'linewidth':2, 'edgecolor':"black"}
tags = sentiment_count
explode = (0.1,0.1,0.1)
tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors = colors,
         startangle=90, wedgeprops = wp, explode = explode, label='')
plt.title('Sentiment Distribution of Tweets by Trump')

#change all needed values to numeric before procedding.

df['Sentiment']= df['Sentiment'].replace("negative", 0)
df['Sentiment']= df['Sentiment'].replace("neutral", 1)
df['Sentiment']= df['Sentiment'].replace("positive", 2)

df['time'] = df['time'].cat.codes

df['time'] = df['time'].astype('int64')

df['day'] = df['day'].astype('int64')

df.info()

dfcorr = pd.DataFrame(df[['likes','retweets','tweetlength','hour','Sentiment','day']])
corr_matrix = dfcorr.corr()
print(corr_matrix)

#create the x and y variables
x = pd.DataFrame(df[['retweets','tweetlength', 'hour','Sentiment', 'day']])
y = df.likes

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


# X is the feature matrix containing the preprocessed tweet text and additional features
# y is the target variable, which is the number of likes for each tweet
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40386014)

# Initialize a linear regression model
lr = LinearRegression()

# Fit the model to the training data
lr.fit(x_train, y_train)

# Make predictions on the test data
y_pred = lr.predict(x_test)

# Calculate regression metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the regression metrics
print("Mean squared error:", mse)
print("Mean absolute error:", mae)
print("R-squared score:", r2)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score


# Create the Random Forest Regression model
rf = RandomForestRegressor(n_estimators=100, random_state=40386014)

# Fit the model on the training data
rf.fit(x_train, y_train)

# Make predictions on the testing data
y_pred = rf.predict(x_test)

# Calculate the mean squared error

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean squared error:", mse)
print("Mean absolute error:", mae)
print("R-squared score for Random Forest Regression:", r2)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# create and fit the model
model = DecisionTreeRegressor(random_state=40386014)
model.fit(x_train, y_train)

# make predictions on the test set
y_pred = model.predict(x_test)

# evaluate the model using r-squared, mean squared error, and mean absolute error
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("R-squared score: {:.2f}".format(r2))
print("Mean squared error: {:.2f}".format(mse))
print("Mean absolute error: {:.2f}".format(mae))